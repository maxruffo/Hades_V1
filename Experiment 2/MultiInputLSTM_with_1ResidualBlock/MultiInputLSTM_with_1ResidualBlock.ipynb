{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T11:55:00.959041300Z",
     "start_time": "2024-11-27T11:54:44.782975400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T11:55:45.585394400Z",
     "start_time": "2024-11-27T11:55:43.301352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  Timestamp      Open      High       Low     Close  \\\n0       2022-01-01 00:00:00  46303.98  46428.08  46300.49  46423.88   \n1       2022-01-01 00:05:00  46423.88  46450.00  46363.23  46431.30   \n2       2022-01-01 00:10:00  46431.30  46467.67  46392.00  46430.04   \n3       2022-01-01 00:15:00  46429.65  46467.67  46406.00  46457.41   \n4       2022-01-01 00:20:00  46457.41  46457.42  46350.67  46397.59   \n...                     ...       ...       ...       ...       ...   \n298348  2024-11-01 23:40:00  69383.99  69455.28  69364.00  69440.00   \n298349  2024-11-01 23:45:00  69440.00  69455.95  69402.00  69438.00   \n298350  2024-11-01 23:50:00  69437.99  69440.00  69404.00  69420.00   \n298351  2024-11-01 23:55:00  69420.00  69447.20  69412.27  69440.01   \n298352  2024-11-02 00:00:00  69440.01  69474.73  69364.00  69474.72   \n\n           Volume            Kline_Close_Time  Quote_Asset_Volume  \\\n0       177.81820  2022-01-01 00:04:59.999000        8.243030e+06   \n1        76.40435  2022-01-01 00:09:59.999000        3.545795e+06   \n2        46.35713  2022-01-01 00:14:59.999000        2.152665e+06   \n3        56.53522  2022-01-01 00:19:59.999000        2.625659e+06   \n4        48.99894  2022-01-01 00:24:59.999000        2.273238e+06   \n...           ...                         ...                 ...   \n298348   30.56136  2024-11-01 23:44:59.999000        2.121544e+06   \n298349   15.75207  2024-11-01 23:49:59.999000        1.093685e+06   \n298350   18.41257  2024-11-01 23:54:59.999000        1.278162e+06   \n298351   11.64643  2024-11-01 23:59:59.999000        8.086257e+05   \n298352   53.13371  2024-11-02 00:04:59.999000        3.688095e+06   \n\n        Number_of_Trades  Taker_Buy_Base_Asset_Volume  ...   BB_middle  \\\n0                   2476                     65.70076  ...         NaN   \n1                   2319                     26.06380  ...         NaN   \n2                   2035                     28.82591  ...         NaN   \n3                   1786                     33.34084  ...         NaN   \n4                   2277                     27.94344  ...         NaN   \n...                  ...                          ...  ...         ...   \n298348              5136                     19.29850  ...  69212.2450   \n298349              4031                      6.84450  ...  69217.9450   \n298350              3014                     10.73884  ...  69226.9450   \n298351              1914                      5.94123  ...  69237.2455   \n298352              6521                     35.52377  ...  69261.8815   \n\n            BB_lower      Slowk      Slowd        ADX      STDDEV  \\\n0                NaN        NaN        NaN        NaN         NaN   \n1                NaN        NaN        NaN        NaN         NaN   \n2                NaN        NaN        NaN        NaN         NaN   \n3                NaN        NaN        NaN        NaN         NaN   \n4                NaN        NaN        NaN        NaN         NaN   \n...              ...        ...        ...        ...         ...   \n298348  68882.971673  97.456640  88.182776  25.933330  152.236268   \n298349  68876.793474  96.914535  95.038321  27.180309  134.789471   \n298350  68874.048555  92.565402  95.645526  28.338536  104.129244   \n298351  68871.683080  95.891329  95.123755  30.533787   79.920604   \n298352  68902.386045  99.996584  96.151105  32.355399   78.493797   \n\n        Ichimoku_Conversion  Ichimoku_Base  Ichimoku_SpanA  Ichimoku_SpanB  \n0                       NaN            NaN             NaN             NaN  \n1                       NaN            NaN             NaN             NaN  \n2                       NaN            NaN             NaN             NaN  \n3                       NaN            NaN             NaN             NaN  \n4                       NaN            NaN             NaN             NaN  \n...                     ...            ...             ...             ...  \n298348            69344.640      69157.250      69185.9250        69667.65  \n298349            69344.975      69155.225      69248.5100        69644.85  \n298350            69344.975      69155.225      69238.0100        69608.07  \n298351            69344.975      69155.225      69230.0100        69608.07  \n298352            69354.365      69164.615      69228.1475        69596.07  \n\n[298353 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Timestamp</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Kline_Close_Time</th>\n      <th>Quote_Asset_Volume</th>\n      <th>Number_of_Trades</th>\n      <th>Taker_Buy_Base_Asset_Volume</th>\n      <th>...</th>\n      <th>BB_middle</th>\n      <th>BB_lower</th>\n      <th>Slowk</th>\n      <th>Slowd</th>\n      <th>ADX</th>\n      <th>STDDEV</th>\n      <th>Ichimoku_Conversion</th>\n      <th>Ichimoku_Base</th>\n      <th>Ichimoku_SpanA</th>\n      <th>Ichimoku_SpanB</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-01 00:00:00</td>\n      <td>46303.98</td>\n      <td>46428.08</td>\n      <td>46300.49</td>\n      <td>46423.88</td>\n      <td>177.81820</td>\n      <td>2022-01-01 00:04:59.999000</td>\n      <td>8.243030e+06</td>\n      <td>2476</td>\n      <td>65.70076</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-01 00:05:00</td>\n      <td>46423.88</td>\n      <td>46450.00</td>\n      <td>46363.23</td>\n      <td>46431.30</td>\n      <td>76.40435</td>\n      <td>2022-01-01 00:09:59.999000</td>\n      <td>3.545795e+06</td>\n      <td>2319</td>\n      <td>26.06380</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-01 00:10:00</td>\n      <td>46431.30</td>\n      <td>46467.67</td>\n      <td>46392.00</td>\n      <td>46430.04</td>\n      <td>46.35713</td>\n      <td>2022-01-01 00:14:59.999000</td>\n      <td>2.152665e+06</td>\n      <td>2035</td>\n      <td>28.82591</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-01 00:15:00</td>\n      <td>46429.65</td>\n      <td>46467.67</td>\n      <td>46406.00</td>\n      <td>46457.41</td>\n      <td>56.53522</td>\n      <td>2022-01-01 00:19:59.999000</td>\n      <td>2.625659e+06</td>\n      <td>1786</td>\n      <td>33.34084</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-01 00:20:00</td>\n      <td>46457.41</td>\n      <td>46457.42</td>\n      <td>46350.67</td>\n      <td>46397.59</td>\n      <td>48.99894</td>\n      <td>2022-01-01 00:24:59.999000</td>\n      <td>2.273238e+06</td>\n      <td>2277</td>\n      <td>27.94344</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298348</th>\n      <td>2024-11-01 23:40:00</td>\n      <td>69383.99</td>\n      <td>69455.28</td>\n      <td>69364.00</td>\n      <td>69440.00</td>\n      <td>30.56136</td>\n      <td>2024-11-01 23:44:59.999000</td>\n      <td>2.121544e+06</td>\n      <td>5136</td>\n      <td>19.29850</td>\n      <td>...</td>\n      <td>69212.2450</td>\n      <td>68882.971673</td>\n      <td>97.456640</td>\n      <td>88.182776</td>\n      <td>25.933330</td>\n      <td>152.236268</td>\n      <td>69344.640</td>\n      <td>69157.250</td>\n      <td>69185.9250</td>\n      <td>69667.65</td>\n    </tr>\n    <tr>\n      <th>298349</th>\n      <td>2024-11-01 23:45:00</td>\n      <td>69440.00</td>\n      <td>69455.95</td>\n      <td>69402.00</td>\n      <td>69438.00</td>\n      <td>15.75207</td>\n      <td>2024-11-01 23:49:59.999000</td>\n      <td>1.093685e+06</td>\n      <td>4031</td>\n      <td>6.84450</td>\n      <td>...</td>\n      <td>69217.9450</td>\n      <td>68876.793474</td>\n      <td>96.914535</td>\n      <td>95.038321</td>\n      <td>27.180309</td>\n      <td>134.789471</td>\n      <td>69344.975</td>\n      <td>69155.225</td>\n      <td>69248.5100</td>\n      <td>69644.85</td>\n    </tr>\n    <tr>\n      <th>298350</th>\n      <td>2024-11-01 23:50:00</td>\n      <td>69437.99</td>\n      <td>69440.00</td>\n      <td>69404.00</td>\n      <td>69420.00</td>\n      <td>18.41257</td>\n      <td>2024-11-01 23:54:59.999000</td>\n      <td>1.278162e+06</td>\n      <td>3014</td>\n      <td>10.73884</td>\n      <td>...</td>\n      <td>69226.9450</td>\n      <td>68874.048555</td>\n      <td>92.565402</td>\n      <td>95.645526</td>\n      <td>28.338536</td>\n      <td>104.129244</td>\n      <td>69344.975</td>\n      <td>69155.225</td>\n      <td>69238.0100</td>\n      <td>69608.07</td>\n    </tr>\n    <tr>\n      <th>298351</th>\n      <td>2024-11-01 23:55:00</td>\n      <td>69420.00</td>\n      <td>69447.20</td>\n      <td>69412.27</td>\n      <td>69440.01</td>\n      <td>11.64643</td>\n      <td>2024-11-01 23:59:59.999000</td>\n      <td>8.086257e+05</td>\n      <td>1914</td>\n      <td>5.94123</td>\n      <td>...</td>\n      <td>69237.2455</td>\n      <td>68871.683080</td>\n      <td>95.891329</td>\n      <td>95.123755</td>\n      <td>30.533787</td>\n      <td>79.920604</td>\n      <td>69344.975</td>\n      <td>69155.225</td>\n      <td>69230.0100</td>\n      <td>69608.07</td>\n    </tr>\n    <tr>\n      <th>298352</th>\n      <td>2024-11-02 00:00:00</td>\n      <td>69440.01</td>\n      <td>69474.73</td>\n      <td>69364.00</td>\n      <td>69474.72</td>\n      <td>53.13371</td>\n      <td>2024-11-02 00:04:59.999000</td>\n      <td>3.688095e+06</td>\n      <td>6521</td>\n      <td>35.52377</td>\n      <td>...</td>\n      <td>69261.8815</td>\n      <td>68902.386045</td>\n      <td>99.996584</td>\n      <td>96.151105</td>\n      <td>32.355399</td>\n      <td>78.493797</td>\n      <td>69354.365</td>\n      <td>69164.615</td>\n      <td>69228.1475</td>\n      <td>69596.07</td>\n    </tr>\n  </tbody>\n</table>\n<p>298353 rows × 30 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Dataset\n",
    "file_path = \".../data/training_data/BTCUSDT/BTCUSDT_2022-01-01_2024-11-01_5.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "LSTMWithResidual(\n",
      "  (lstm_y): LSTM(1, 64, batch_first=True)\n",
      "  (lstm_p): LSTM(2, 64, batch_first=True)\n",
      "  (lstm_n): LSTM(2, 64, batch_first=True)\n",
      "  (residual): ResidualBlock(\n",
      "    (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (norm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation1): ReLU()\n",
      "    (linear2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation2): ReLU()\n",
      "    (downsample): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Number of trainable parameters:\n",
      "60,577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 170\u001B[0m\n\u001B[0;32m    168\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model(Y, X_p, X_n)\n\u001B[0;32m    169\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(predictions\u001B[38;5;241m.\u001B[39msqueeze(), y_target\u001B[38;5;241m.\u001B[39msqueeze())\n\u001B[1;32m--> 170\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    172\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AKI\\Hades_V1\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AKI\\Hades_V1\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\AKI\\Hades_V1\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Feature-Auswahl\n",
    "features = ['SMA_50', 'SMA_200', 'EMA_50', 'EMA_200', 'MACD_Signal']\n",
    "target = 'Close'\n",
    "\n",
    "# Daten vorbereiten\n",
    "prices = df['Close'].values.reshape(-1, 1)\n",
    "\n",
    "X = df[features].values.copy()\n",
    "y = df[target].values.copy()\n",
    "\n",
    "scaler_X = RobustScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = RobustScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "train_size = int(len(X_scaled) * 0.7)\n",
    "X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]\n",
    "y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]\n",
    "\n",
    "# Funktion zur Sequenz-Erstellung\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X[i:i + sequence_length])\n",
    "        y_seq.append(y[i + sequence_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "sequence_length = 60\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "\n",
    "# Dataset und DataLoader für PyTorch\n",
    "class MultiInputTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx]\n",
    "        y_target = self.y[idx]\n",
    "        Y = X_seq[:, 0].unsqueeze(-1)\n",
    "        X_p = X_seq[:, 1:3]\n",
    "        X_n = X_seq[:, 3:5]\n",
    "        return Y, y_target, X_p, X_n\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = MultiInputTimeSeriesDataset(X_train_seq, y_train_seq, sequence_length)\n",
    "test_dataset = MultiInputTimeSeriesDataset(X_test_seq, y_test_seq, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, activation=nn.ReLU, normalization=nn.BatchNorm1d):\n",
    "      \"\"\"\n",
    "      Residual block with skip connections.\n",
    "      Args:\n",
    "          input_dim (int): Dimension of the input features.\n",
    "          hidden_dim (int): Dimension of the hidden layer.\n",
    "          activation (callable): Activation function class (default: ReLU).\n",
    "          normalization (callable): Normalization layer class (default: BatchNorm1d).\n",
    "      \"\"\"\n",
    "      super(ResidualBlock, self).__init__()\n",
    "      self.input_dim = input_dim\n",
    "      self.hidden_dim = hidden_dim\n",
    "\n",
    "      # First layer: Linear -> Normalization -> Activation\n",
    "      self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "      self.norm1 = normalization(hidden_dim)\n",
    "      self.activation1 = activation()\n",
    "\n",
    "      # Second layer: Linear -> Normalization -> Activation\n",
    "      self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "      self.norm2 = normalization(input_dim)\n",
    "      self.activation2 = activation()\n",
    "\n",
    "      # Shortcut connection: Optional downsampling if dimensions don't match\n",
    "      self.downsample = (\n",
    "          nn.Linear(input_dim, input_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"\n",
    "      Forward pass through the residual block.\n",
    "      Args:\n",
    "          x (Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "      Returns:\n",
    "          Tensor: Output tensor of the same shape as input.\n",
    "      \"\"\"\n",
    "      # Store the original input for the skip connection\n",
    "      residual = self.downsample(x)\n",
    "\n",
    "      # Pass through the first layer\n",
    "      out = self.linear1(x)\n",
    "      out = self.norm1(out)\n",
    "      out = self.activation1(out)\n",
    "\n",
    "      # Pass through the second layer\n",
    "      out = self.linear2(out)\n",
    "      out = self.norm2(out)\n",
    "\n",
    "      # Add the residual (skip connection) and apply the activation\n",
    "      out += residual\n",
    "      out = self.activation2(out)\n",
    "\n",
    "      return out\n",
    "\n",
    "# MultiInputLSTM mit Residual Layer\n",
    "class LSTMWithResidual(nn.Module):\n",
    "    def __init__(self, input_size_pn, hidden_size, residual_dim):\n",
    "        super(LSTMWithResidual, self).__init__()\n",
    "        self.lstm_y = nn.LSTM(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm_p = nn.LSTM(input_size=input_size_pn, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm_n = nn.LSTM(input_size=input_size_pn, hidden_size=hidden_size, batch_first=True)\n",
    "        self.residual = ResidualBlock(hidden_size, residual_dim)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, Y, X_p, X_n):\n",
    "        Y_out, _ = self.lstm_y(Y)\n",
    "        X_p_out, _ = self.lstm_p(X_p)\n",
    "        X_n_out, _ = self.lstm_n(X_n)\n",
    "        combined = Y_out[:, -1, :] + X_p_out[:, -1, :] + X_n_out[:, -1, :]\n",
    "        combined_residual = self.residual(combined)\n",
    "        output = self.fc(combined_residual)\n",
    "        return output\n",
    "\n",
    "# Modell initialisieren\n",
    "input_size_pn = 2\n",
    "hidden_size = 64\n",
    "residual_dim = 32\n",
    "\n",
    "model = LSTMWithResidual(input_size_pn=input_size_pn, hidden_size=hidden_size, residual_dim=residual_dim)\n",
    "\n",
    "def model_summary(model):\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print(\"\\nNumber of trainable parameters:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{total_params:,}\")\n",
    "\n",
    "# Gebe die Modellzusammenfassung aus\n",
    "model_summary(model)\n",
    "\n",
    "\n",
    "# Training und Optimierung\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for Y, y_target, X_p, X_n in train_loader:\n",
    "        Y, y_target, X_p, X_n = Y.to(device), y_target.to(device), X_p.to(device), X_n.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(Y, X_p, X_n)\n",
    "        loss = criterion(predictions.squeeze(), y_target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Y, y_target, X_p, X_n in test_loader:\n",
    "            Y, y_target, X_p, X_n = Y.to(device), y_target.to(device), X_p.to(device), X_n.to(device)\n",
    "            predictions = model(Y, X_p, X_n)\n",
    "            loss = criterion(predictions.squeeze(), y_target.squeeze())\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"MultiInputLSTM_ResidualBlock.pth\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "# Beste Modell laden\n",
    "model.load_state_dict(torch.load(\"MultiInputLSTM_ResidualBlock.pth\"))\n",
    "\n",
    "# Vorhersagen\n",
    "model.eval()\n",
    "train_predictions, test_predictions = [], []\n",
    "with torch.no_grad():\n",
    "    for Y, y_target, X_p, X_n in train_loader:\n",
    "        Y, X_p, X_n = Y.to(device), X_p.to(device), X_n.to(device)\n",
    "        outputs = model(Y, X_p, X_n)\n",
    "        train_predictions.extend(outputs.cpu().numpy())\n",
    "    for Y, y_target, X_p, X_n in test_loader:\n",
    "        Y, X_p, X_n = Y.to(device), X_p.to(device), X_n.to(device)\n",
    "        outputs = model(Y, X_p, X_n)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "train_predictions = scaler_y.inverse_transform(np.array(train_predictions).reshape(-1, 1))\n",
    "test_predictions = scaler_y.inverse_transform(np.array(test_predictions).reshape(-1, 1))\n",
    "y_train_unscaled = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1))\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"../Evaluation_PNGs/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Folder '{output_dir}' created for saving the plots.\")\n",
    "\n",
    "# Visualisierung der Ergebnisse\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_unscaled, label='Tatsächliche Preise')\n",
    "plt.plot(test_predictions, label='Vorhergesagte Preise')\n",
    "plt.xlabel('Zeit')\n",
    "plt.ylabel('Preis')\n",
    "plt.title('Tatsächliche vs. Vorhergesagte Preise (PyTorch Modell)')\n",
    "plt.legend()\n",
    "\n",
    "# Save plot\n",
    "price_plot_path = os.path.join(output_dir, \"MultiInputLSTM_ResidualBlock_price_predictions.png\")\n",
    "plt.savefig(price_plot_path)\n",
    "print(f\"Saved price predictions plot to {price_plot_path}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot der Trainings- und Validierungsverluste\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Trainings-Loss')\n",
    "plt.plot(val_losses, label='Validierungs-Loss')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Trainings- und Validierungs-Loss Verlauf')\n",
    "plt.legend()\n",
    "\n",
    "# Save plot\n",
    "loss_plot_path = os.path.join(output_dir, \"MultiInputLSTM_ResidualBlock_loss_curves.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "print(f\"Saved loss curves plot to {loss_plot_path}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T11:56:16.567182900Z",
     "start_time": "2024-11-27T11:55:45.571426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
